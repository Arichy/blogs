# 测试时间

2026年1月30日

# Prompt

设计并开发一个基于 OpenAI API 的产品瑕疵检测系统，该系统应允许用户上传产品图片，通过调用 OpenAI API 分析图片内容，识别并以文字输出图片上存在的瑕疵。具体功能需求包括：实现图片上传界面，支持常见图片格式（JPG、PNG等）；集成 OpenAI API 进行图像分析，要求能够准确识别至少5种常见产品瑕疵类型（如划痕、凹陷、色差、污渍、变形等）；提供清晰的检测结果展示，包括瑕疵位置标记、瑕疵类型分类及置信度评分；系统需具备错误处理机制，能够处理图片上传失败、API调用超时等异常情况；界面设计应简洁直观，响应式布局适配不同设备. 有什么需要我做抉择的可以随时问我

# 简单分析

这个任务唯一的难点在于如何调用 OpenAI API, 以及生成的 prompt, 别的都是非常常规的业务逻辑, 所以我们重点会关注 AI 生成的 prompt 的质量. 由于是调用 OpenAI API 来检测瑕疵, 最终的效果由 OpenAI 决定, 所以评分时**不会关心瑕疵检测的准确性**. 所以总的来说会关注以下几部分:

1. prompt 的质量 (50%)
2. UI 的美观程度和功能 (40%)
3. 开发的流畅程度 (10%)

# 1. Claude Opus 4.5 (In Cursor)

## prompt

```
你是一个专业的产品质量检测AI助手。你的任务是分析产品图片，识别其中可能存在的瑕疵和质量问题。

你需要检测以下类型的瑕疵：
划痕 (Scratch) - 表面的线性损伤或刮痕
凹陷 (Dent) - 表面的凹陷或压痕
色差 (Color Deviation) - 颜色不均匀、斑点或与预期颜色不符
污渍 (Stain) - 表面的污点、水渍或其他污染
变形 (Deformation) - 形状扭曲、弯曲或不规则
裂纹 (Crack) - 表面或结构的裂缝
气泡 (Bubble) - 材料内部或表面的气泡
毛刺 (Burr) - 边缘粗糙或毛边

请以JSON格式返回分析结果，格式如下：
{
  "overallQuality": "优秀/良好/一般/较差",
  "qualityScore": 0-100的数字,
  "defects": [
    {
      "type": "瑕疵类型（中文）",
      "typeEn": "瑕疵类型（英文）",
      "location": "瑕疵在图片中的位置描述",
      "severity": "严重程度：轻微/中等/严重",
      "confidence": 0-100的置信度数字,
      "description": "详细描述"
    }
  ],
  "summary": "整体检测总结，简洁描述产品状况",
  "recommendations": ["改进建议1", "改进建议2"]
}

注意事项：
如果图片不是产品图片或无法识别，请在summary中说明
如果没有检测到瑕疵，defects数组为空，qualityScore应该较高
置信度表示你对该瑕疵判断的确定程度
位置描述要具体，如"左上角"、"中心偏右"、"底部边缘"等
请只返回JSON，不要包含其他文字
```

总体来说 prompt 质量还不错, 唯一的缺点在于, 我们的需求 prompt 里明确说了 UI 上需要展示瑕疵的位置, 但是 Claude 生成的 prompt 里没有包含这个要求, 所以在生成代码的时候, Claude 生成的 UI 上没有展示瑕疵的位置, 这是一个扣分项.

**Prompt 得分: 7.**

## UI

![Claude Opus 4.5 (In Cursor)](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/1.png?raw=true)

UI 现代且简洁, 配色舒适, 用了环形图展示评分, 列出了检测摘要, 还有改进建议, 唯一的缺点还是上面提到的没有展示瑕疵的位置.

**UI 得分: 7.**

## 开发体验

Claude 的开发体验很少让人失望过, 对于这种简单的任务一次过是基本操作, 直接满分.

**开发体验得分: 10.**

## 综合评分

**Claude Opus 4.5 (In Cursor): 7.3**

# 2. Claude Opus 4.5 (In Claude Code)

## Prompt

```
你是一个专业的产品质量检测专家。请仔细分析这张产品图片，识别以下类型的瑕疵：
划痕 (Scratch) - 表面线性损伤
凹陷 (Dent) - 表面凹陷变形
色差 (Color Deviation) - 颜色不均匀或异常
污渍 (Stain) - 表面污染物
变形 (Deformation) - 形状异常
裂纹 (Crack) - 表面裂缝
气泡 (Bubble) - 表面气泡缺陷

请严格按照以下JSON格式返回检测结果，不要包含任何其他文字：
{
  "hasDefects": boolean,
  "defects": [
    {
      "type": "瑕疵类型中文名",
      "typeEn": "DefectTypeInEnglish",
      "location": "瑕疵在图片中的位置描述，如：左上角、中部偏右、底部边缘等",
      "severity": "low或medium或high",
      "confidence": 0.0到1.0之间的数字,
      "description": "详细描述该瑕疵的特征"
    }
  ],
  "overallQuality": "good或acceptable或poor",
  "summary": "对产品整体质量的总结评估"
}

注意：
如果没有发现瑕疵，hasDefects设为false，defects为空数组
confidence表示你对该瑕疵判断的置信度
severity根据瑕疵对产品质量的影响程度判断
请仔细观察图片的每个区域
```

和 Cursor 里的结果差不多, 所以评分也保持一致.

**Prompt 得分: 7.**

## UI

![Claude Opus 4.5 (In Claude Code)](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/2.png?raw=true)

这次 Claude 生成的 UI 就明显不如 Cursor 版的. 在点击分析后不仅没有展示上传的图片, 而且整体设计也比较花里胡哨, 背景色太杂, 还有 AI 特有的喜欢用 emoji.

**UI 得分: 5.**

## 开发体验

Claude Code 不用多说, 一次过.

**开发体验得分: 10.**

## 综合评分

**Claude Opus 4.5 (In Claude Code): 6.5**

# 3. GPT-5.2-codex

## Prompt

```
"你是产品瑕疵检测助手，请只输出严格 JSON。",
"需要识别至少以下5种瑕疵类型：划痕、凹陷、色差、污渍、变形。",
"输出结构如下：",
"{",
'  "summary": "一句话总结",',
'  "defects": [',
"    {",
'      "type": "划痕|凹陷|色差|污渍|变形",',
'      "confidence": 0.0-1.0,',
'      "bbox": { "x": 0-1, "y": 0-1, "width": 0-1, "height": 0-1 }',
"    }",
"  ]",
"}",
"如果没有瑕疵，defects 为空数组，并说明 summary。",
"bbox 为归一化坐标，基于图像宽高比例，粗略框出瑕疵位置。"
```

这个 prompt 相比 Claude 来说明显拉了一大截. 虽说 context 不宜过长, 但是这也太简洁了.

**Prompt 得分: 4**

## UI

![GPT-5.2-codex](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/3.png?raw=true)

Prompt 简陋, UI 也简陋, 检测结果就只简单写出了两个字. 不过在结果图片里标注了瑕疵位置(开头提到过, 不关心结果准确度, 因为这不 Coding Agent 主要决定的).

**UI 得分: 5**

## 开发体验

说实话 GPT 这块真的挺让我震惊, 因为它是唯一一个调用 OpenAI API 报错的 AI, 自己调用自己的 API 都能因为传了不存在的参数而报错也是挺离谱的, 还报错了 3 轮.

**开发体验得分: 5**

## 综合评分

**GPT-5.2-codex: 4.5**

# 4. Gemini 3 Pro

## Prompt

```
You are an expert Quality Assurance Inspector for product manufacturing.
Your task is to analyze product images and identify potential defects.
Focus on these specific defect types:
Scratches (划痕)
Dents (凹陷)
Discoloration (色差)
Stains (污渍)
Deformation (变形)

For each defect found, provide:
type: One of the 5 types listed above.
confidence: A score between 0 and 1 indicating your certainty.
description: A brief description of the defect.
location: Approximate center coordinates of the defect as [x, y] percentages (0-100) relative to the image width and height. Top-left is [0, 0].

Return the result as a JSON object with a "defects" array.
Example format:
{
  "defects": [
    {
      "type": "Scratches",
      "confidence": 0.95,
      "description": "Visible scratch on the top surface",
      "location": [50, 25]
    }
  ]
}

If no defects are found, return {"defects": []}.
```

Prompt 和 Claude 比较类似, 并且说明了 location 需要是一个坐标, 加分.

**Prompt 得分: 8**

## UI

![Gemini 3 Pro](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/4.png?raw=true)

号称"前端严父"的 Gemini 做的 UI 也不怎么样, 过于简陋, 不过好在标注了瑕疵的位置, 并且位置是最准的一个(尽管这更多是 Openai 的功劳).

**UI 得分: 6**

## 开发体验

一次过.

**开发体验得分: 10**

## 综合评分

**Gemini 3 Pro: 7.4**

# 5. Composer-1

## Prompt

```
You are a quality control expert analyzing product images for defects.
Analyze the image and identify any defects from these categories:
- scratch: Surface scratches or marks
- dent: Dents or depressions
- color_difference: Color inconsistencies or variations
- stain: Stains, spots, or discolorations
- deformation: Shape deformations or warping
- crack: Cracks or fractures
- discoloration: Unusual color changes
- surface_imperfection: Other surface imperfections

For each defect found, provide:
1. Type (from the categories above)
2. Location description (e.g., "top-left corner", "center area", "edge")
3. Confidence score (0-100)
4. Description of the defect

Return your response as a JSON object with this structure:
{
"defects": [
  {
    "type": "scratch",
    "location": "top-left corner",
    "confidence": 85,
    "description": "Visible scratch approximately 2cm long"
  }
],
"summary": "Overall quality assessment",
"overallQuality": "good|fair|poor"
}

If no defects are found, return an empty defects array with overallQuality "good".
```

和 Claude 非常像, 同样很详细清楚, 同样缺少了对具体位置的要求, 只要求了模糊位置.

**Prompt 得分: 7**

## UI

![Composer-1](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/5.png?raw=true)

UI 相比 Claude 就差了很多.

- `Analyze Another Image` 按钮颜色有点突兀
- 没有在图里画出位置
- 检测结果里的 `90% confidence` 和 `Stain` 感觉应该互换颜色
- 具体瑕疵的文字颜色比较浅, 不醒目

**UI 得分: 5**

## 开发体验

一次过.

**开发体验得分: 10**

## 综合评分

**Composer-1: 6.5**

# 6. Kimi-K2.5

## Prompt

```
你是一个专业的产品质量检测专家。请仔细分析产品图片，识别以下5种常见瑕疵类型：
划痕 (scratch) - 产品表面的线性损伤
凹陷 (dent) - 产品表面的局部下陷
色差 (color_difference) - 颜色不均匀或与标准不符
污渍 (stain) - 表面的污染或斑点
变形 (deformation) - 产品形状异常或扭曲

请以 JSON 格式返回分析结果，格式如下：
{
  "defects": [
    {
      "type": "瑕疵类型英文",
      "name": "瑕疵中文名称",
      "location": "瑕疵位置描述",
      "confidence": 0.0-1.0,
      "description": "详细描述",
      "severity": "轻微/中等/严重"
    }
  ],
  "summary": "总体评估摘要",
  "totalDefects": 0
}

注意：
只返回 JSON 数据，不要其他文字说明
如果没有发现瑕疵，defects 为空数组
confidence 为 0-1 之间的浮点数
severity 只能是 "轻微"、"中等" 或 "严重"`
```

同样的, 没有写明 location 需要具体坐标.

**Prompt 得分: 7**

## UI

![Kimi-K2.5](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/6.png?raw=true)

Kimi 生成的 UI 是 AI 味最浓的: 经典蓝紫色渐变, 经典 emoji, 经典黄色背景文字框.

**UI 得分: 6**

## 开发体验

Kimi 在写代码的时候没有出什么乱子, 但是在运行的时候报了依赖找不到的错误, 应该是执行安装依赖的命令时出了什么错.

**开发体验得分: 6**

## 综合评分:

**Kimi-K2.5: 6.5**

# 7. Qwen-3-coder

本次挑战最大战犯登场.

## Prompt

```
请分析这张产品图片，识别以下5种类型的瑕疵：划痕、凹陷、色差、污渍、变形。对于每个发现的瑕疵，请提供：1)瑕疵类型 2)在图片中的位置描述 3)置信度评分(0-100%)。如果未发现明显瑕疵，请回复'未发现明显瑕疵'。以JSON格式返回结果，包含字段：defects(数组，每个元素包含type、location、confidence)。
```

- Prompt 没有换行
- 没有提供 JSON 形式的说明
- 虽然写了需要什么字段, 但是只写了字段名, 没有写字段类型和描述

**Prompt 得分: 2**

## UI

![Qwen-3-coder](https://github.com/Arichy/blogs/blob/main/docs/coding-agent-comparison/imgs/2/7.png?raw=true)

没啥好说的, 根本用不了.

- 最上面那个蓝色的方块是上传按钮, 初始文本是 "点击上传", 结果上传之后就变成了文件名, 很匪夷所思的行为
- "开始分析"和"重新选择", 一绿一灰, 非常不协调, 灰色一般用于 disabled 的样式
- 大黑 banner 很土很丑, 像 10 年前的 bootstrap 风格
- 同样没有在图里标明位置

**UI 得分: 1**

## 开发体验

结果都这样了, 开发体验也不重要了.

**开发体验得分: 1**

## 综合评分

**Qwen-3-coder: 1.5**

# 排名

| 模型                                 | Prompt (50%) | UI (40%) | 开发体验 (10%) | 综合评分 |
| :----------------------------------- | :----------: | :------: | :------------: | :------: |
| **Gemini 3 Pro**                     |      8       |    6     |       10       | **7.4**  |
| **Claude Opus 4.5 (In Cursor)**      |      7       |    7     |       10       | **7.3**  |
| **Claude Opus 4.5 (In Claude Code)** |      7       |    5     |       10       | **6.5**  |
| **Composer-1**                       |      7       |    5     |       10       | **6.5**  |
| **Kimi-K2.5**                        |      7       |    6     |       6        | **6.5**  |
| **GPT-5.2-codex**                    |      4       |    5     |       5        | **4.5**  |
| **Qwen-3-coder**                     |      2       |    1     |       1        | **1.5**  |

# 总结

1. Claude: 符合最强的地位. 虽然本次挑战里两次都没有注意到对坐标的要求导致惜败 Gemini, 但是别的方面基本都拉满了.
2. Gemini: 号称前端严父, 但是本次挑战里 UI 画得真的很差, 不过因为抓到了坐标这个需求功能点, 整体小胜 Claude 排到了第一.
3. GPT: 不符合预期. 虽然我平时没有怎么用过 GPT-5.2-codex 这个模型, 但是就仅有的个位数的次数体验里, 还是比较好的, 没有什么问题. 但是在本次挑战中暴露了严重的幻觉, 连自己的 API 都不知道有什么参数.
4. Composer-1: 很多人可能没听过这个模型, 这是 Cursor 官方推出的第一个自研编程模型, 最大的特点是速度真的快到飞起. 刚出来的时候我用它问过几个问题, 我的眼睛完全跟不上它往外吐 token 的速度, 一瞬间就完成了回复. 所以一直想测一下它的编程质量如何. 由于是 Cursor 自己出的第一个模型, 我其实是对它没报啥希望的, 但是最终呈现的效果还不错, 中规中矩地完成了, 所以整体超出预期.
5. Kimi-K2.5: 我之前一直不太喜欢 Kimi, 因为这家公司一直感觉营销大于实力, 尤其是 Kimi-K2 刚出来的时候, 全网都在刷如何用它来驱动 Claude Code, 所以我对它同样没啥期待, 但是最终的效果也还不错, 虽然生成 UI 的 AI 味很浓.
6. Qwen-3-coder: 有点过分了. 我一直以为它的能力是国产 AI 里第一梯队的, 没想到居然表现这么差, 全方位地差. 只能说继续加油吧☹️.
